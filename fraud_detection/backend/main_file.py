# -*- coding: utf-8 -*-
"""main_file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JRzlZ196LvXCWo6DTAIF1iGCspEIpjV3
"""

import re
import torch
from nltk.corpus import words
import nltk
from nltk.tokenize import word_tokenize
from torch.nn.utils.rnn import pad_sequence
from nltk.corpus import stopwords, words as nltk_words
from torch.nn.utils.rnn import pad_sequence
import json
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('words')

#load vocab
with open('vocab.json', 'r') as file:
    vocab = json.load(file)

#define function to clean statement
def preprocess_string(s):
    s = re.sub(r"[^\w\s]", '', s)
    s = re.sub(r"\s+", ' ', s)     # Replace multiple spaces with a single space
    s = re.sub(r"\d", '', s)       # Remove digits
    return s.strip()

english_words = set(words.words())
stop_words = set(stopwords.words('english'))

def remove_stop_words(text):
    # Tokenize the text
    tokens = word_tokenize(text)

    # Filter out the stop words
    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]

    # Join the tokens back into a string
    return ' '.join(filtered_tokens)
def remove_non_english_words(text):
    # Extract words from the text using regex
    words_in_text = re.findall(r'\b\w+\b', text)

    # Filter out non-English words
    english_only = [word for word in words_in_text if word.lower() in english_words]

    # Join the filtered words back into a string
    return ' '.join(english_only)

# define a function to process user's input

def process_single_text(text, vocab, max_sequence_length=350, unk_index=None):


    # Preprocess the input text (cleaning, removing non-English words, and stopwords)
    text = preprocess_string(text)
    text = remove_non_english_words(text)
    text = remove_stop_words(text)

    # Tokenize the preprocessed text
    tokens = word_tokenize(text.lower())

    # Convert tokens to their corresponding indices in the vocab, using <unk> for unknown words
    if unk_index is None:
        unk_index = vocab.get("<unk>", len(vocab))

    token_indices = [vocab.get(token, unk_index) for token in tokens]

    # Pad or truncate the tokenized sequence
    if len(token_indices) > max_sequence_length:
        token_indices = token_indices[:max_sequence_length]
    else:
        # Pad with 0 (assuming <pad> is mapped to index 0)
        token_indices.extend([0] * (max_sequence_length - len(token_indices)))

    # Convert the sequence to a tensor
    processed_tensor = torch.tensor(token_indices, dtype=torch.long)

    return processed_tensor

#Define model structure

import torch
from torch.utils.data import DataLoader, TensorDataset
import torch.nn as nn
import torch.optim as optim

class LSTMDropoutClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, num_layers, dropout):
        super(LSTMDropoutClassifier, self).__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)  # Embedding layer

        # First LSTM layer
        self.lstm1 = nn.LSTM(embedding_dim, hidden_dim, num_layers=1, batch_first=True)
        self.dropout1 = nn.Dropout(dropout)  # Dropout after first LSTM

        # Second LSTM layer
        self.lstm2 = nn.LSTM(hidden_dim, hidden_dim, num_layers=1, batch_first=True)
        self.dropout2 = nn.Dropout(dropout)  # Dropout after second LSTM

        # Fully connected layer
        self.fc1 = nn.Linear(hidden_dim, 64)  # First dense layer
        self.relu = nn.ReLU()  # ReLU activation

        # Final classification layer
        self.fc2 = nn.Linear(64, output_size)  # Final output layer with 24 classes

    def forward(self, x):
        # Embedding
        x = self.embedding(x)

        # First LSTM layer
        lstm_out, _ = self.lstm1(x)
        lstm_out = self.dropout1(lstm_out)

        # Second LSTM layer
        lstm_out, _ = self.lstm2(lstm_out)
        lstm_out = self.dropout2(lstm_out)

        # Get the final hidden state (from the last time step)
        final_hidden_state = lstm_out[:, -1, :]  # Get the last output of the LSTM

        # Pass through dense layers
        out = self.fc1(final_hidden_state)
        out = self.relu(out)
        out = self.fc2(out)

        return out

#Load model
BATCH_SIZE = 32
embedding_dim = 128  # Embedding size
hidden_dim = 256  # Hidden size for LSTM
output_size = 24  # Number of output classes (24 in your case)
num_layers = 2  # Number of LSTM layers
dropout = 0.2  # Dropout rate
vocab_size = len(vocab) + 1
max_sequence_length=350


loaded_model = LSTMDropoutClassifier(vocab_size, embedding_dim, hidden_dim, output_size, num_layers, dropout)
loaded_model.load_state_dict(torch.load("deeper_bilstm_with_dropout.pth"))
loaded_model.eval()